Metadata-Version: 2.1
Name: openlabcluster
Version: 0.0.36
Summary: OpenLabCluster
Home-page: https://github.com/shlizee/OpenLabCluster
Author: Jingyuan Li
Author-email: jingyli6@uw.edu
Keywords: python,first package
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Education
Classifier: Programming Language :: Python :: 2
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: MacOS :: MacOS X
Classifier: Operating System :: Microsoft :: Windows
Description-Content-Type: text/markdown
Provides-Extra: gui
License-File: LICENSE

# OpenLabCluster

## Usage
### Installation
#### Install from pip
Create a new environment with conda and install the package


For **Linux** start the environment with [spec-file.txt](https://drive.google.com/file/d/1nlOqspBrnl5kiErudW6NtHTt3nFCTPJH/view?usp=sharing)

	conda create --name OpenLabCluster --file spec-file.txt
	conda activate OpenLabCluster
	pip install openlabcluster
	
for **Mac OS** and **Windows**

	conda create --name OpenLabCluster python=3.7
	conda activate OpenLabCluster
	pip install 'openlabcluster[gui]'
	 
	
<!---##### Troubleshooting for Linux installation
If, for some reason, `wxPython` fails to install on Linux, run `sudo apt install libsdl2-dev build-essential libgtk-3-dev make gcc libgtk-3-dev libwebkitgtk-dev libwebkitgtk-3.0-dev libgstreamer-gl1.0-0 freeglut3 freeglut3-dev python-gst-1.0 python3-gst-1.0 libglib2.0-dev ubuntu-restricted-extras libgstreamer-plugins-base1.0-dev`. Then, use `pip install -U -f https://extras.wxpython.org/wxPython4/extras/linux/gtk3/<your operating system>  wxPython` to install it. To determine your exact link, go to https://extras.wxpython.org/wxPython4/extras/linux/gtk3/ and select the right folder (copy the link from the search bar). 
	
#### Install the Required Package from Environment File	
Git clone the entire package
Create a new  enviornment.yml file
If you are using **Linux**
	
	conda env create -f environment.yml
	
if you are using **Mac-os**
	
	conda env create -f env-mac2.yml
--->
	
	
### Execution

Run the following for **Linux**
	
	python -m openlabcluster

Run the following for **Mac-OS**

	conda install python.app
	pythonw -m openlabcluster

Run the following for **Windows**

	pythonw.exe -m openlabcluster
		
### Run a Demo

#### Create Demo Project
1. Download the *openlabcluster_example* folder from [here](https://drive.google.com/file/d/1UYtgbnTRrTQOtSAQUC0otix6RMpDvfYs/view?usp=sharing)
1. Go to *your_download-dir/openlabcluster_example* folder run 
		
		python3 prepare_video_list.py
   which generates video_segmetns_names.text file.
   
2. Launch OpenLabCluster GUI (see Execution above)
3. Set Project Name: e.g., demo
4. Click *Load Preprocessed Keypoints*, choose datafile: your_download_dir/openlabcluster_example/demo_data.h5
5. Click *Load Video Segments Name List*, choose the file: your_download_dir/openlabcluster_example/video_segmetns_names.text
6. Uncheck *Check to use GPU*, if GPU is not available.
7. Set Feature Length = 16
8. Click *OK* to create the project

#### Start the Demo Project:
1. Go to **Manage Project** panel, 
2. Choose *Load Project*
3. Select the config file as *filedirectory/project_name/config.yaml*
4. Click **OK**, then go to **Cluster Map**


#### Cluster Map:
1. Click **Start Clustering** button, and start unsupervised clustering. 
2. Click **Go To Classification**, then go to **Behavior Classification Map** panel.

#### Behavior Classification Map: 
1. Behavior Classification Map is initialized on the bottom left, with suggested samples for annotation.
2. Label samples on the bottom right panel.
3. Click **Run Classification**, and start classification.


### Manage Project (Start a New Project or Load Earlier Project) Detailed Description
#### Start a New Project
1. Project Name - the name for the project
2.  If you have only videos, use markerless pose estimators (e.g. DeepLabCut) to extract keypoints. If you already have DeepLabCut-like formatted files, select them with "Load Keypoints Data".
3. Select your videos with "Load Video Segments Names List". There should be one video for each keypoint file. Make sure the videos and keypoints files are in the same order.
4. Optional: Set a directory for the project (the default is the working directory of this project).
5. Keep the GPU box checked if you have a GPU on your computer and you would like to use it for training.
6. Enter the features length (number of body parts * number of dimensions per body part. For example, for 5 keypoints in 2D, this would be 5*2 =10).
7. Choose "OK" to create the project.
8. If you would like to edit the config (i.e. change class names, change video cropping), press "Edit Config File".

#### Loading a Project
1. Select config.yaml file, generated when you created that project.
2. Press "OK".


### Cluster Map (Unsupervised Learning)
#### Set the Training Parameters
1. The setting "Update Cluster Map Every (Epochs)": helps you to decide when to update the Cluster Map (bottom left panel in the figure), e.g. set 1 to update Cluster Map every training epoch, or 5 to update every 5 epochs.
2. The setting "Save Cluster Map Every (Epochs)": specifies when to update and save Cluster Map.
3. The setting "Maximum Epochs": denotes the number of epochs to be performed in full training.
4. The setting "Cluster Map Dimension": allows you to choose between "2d" or "3d". For "2d" the Cluster Map will be shown in 2D, otherwise, it is 3D dimension.
5. Dimension Reduction Methods: possible choices are "PCA", "tSNE", "UMAP". The GUI will use the chosen method to perform dimension reduction and show results in Cluster Map.


#### Buttons
After setting the parameters you can perform the analysis:

1. Start Clustering: perform an unsupervised sequence regeneration task.
2. Stop Clustering: usually, the clustering will stop when it reaches the maximum epochs, but if you want to stop at an intermediate stage, click this button.
3. Continue Clustering: if you stopped the clustering at some stage and want to perform clustering with earlier clustering results, click this button.
4. Go to Classification: after the unsupervised clustering we go to the next step which includes: i) annotation suggestion, ii) sample annotating and iii) semi-supervised action classification with labeled samples.

### Behavior Classification Map
#### Set the Training Parameters
1. Selection Method: In this part, your selection will decide which method GUI uses to select samples for annotation. there are four possible choices ("Marginal Index (MI)", "Core Set (CS)", "Cluster Center (TOP)", "Cluster Random (Rand)", "Uniform").
2. \# Samples per Selection: the number of samples you want to label in the current selection stage. You can select or deselect samples in the behavior classification map.
3. Maximum Epochs: the maximum epoch the network will be trained when performing the action recognition.
4. Cluster Map Dimension: you can choose "2d" or "3d" if it is "2d" the Cluster Map will be shown in 2D dimension, otherwise it is 3D dimension.
5. Dimension Reduction Method: possible choices are "PCA", "tSNE", "UMAP". The GUI will use the chosen reduction method to perform dimension reduction and show results in the Cluster Map.


#### Buttons:
	
1. Run Classification: save annotation results and train the action recognition model.
	
2. Stop Classification: stop training.
	
3. Next Selection: suggest a new set of samples based on with indicated active learning method. **Notice**: if you change Cluster Map Dimension or Dimension Reduction Method, click the **Next Selection** to show suggested samples.
	
4 Get Results: get the Behavior Classification Map (predicted class label) from the trained model on unlabeled samples. 

#### Plots
1. Behavior Classification Plot:
 	
	**Visualization mode**: The points representing each action segment is shown in black for visualization, it can be visualized in 2D or 3D with different dimension reduction method.
 
 	**Annotation mode**: In the mode, dots for each action segment in a different color in the Behavior Classification plot (only in 2D).
		Red: current sample for annotating and the video segment is shown on the right. 
		Blue: the suggested samples for annotating in this iteration (deselect them by clicking the point).
		Green: samples have been annotated.

	Buttons:
	
	  * Zoom: zoom in or zoom out the plot.
	  * Pan: move the plot around.
	
	
2. Videos Panel:
	
	Left panel: The corresponding video of the action segment which is shown in the Behavior Classification Map in red.
	
	Right panel: The class name and class id. According to the video, one can label the action segment.

	Buttons:

	  * Previous: load the previous video.
	  * Play: play the video.
	  * Next: go to the next video.


