# AUTOGENERATED! DO NOT EDIT! File to edit: database2.ipynb (unless otherwise specified).

__all__ = ['DEFAULTCOLS', 'PandasDataFrameAttribute', 'Database', 'updateWithDfs', 'lambdaPresignUpload',
           'lambdaIngestUpload', 'lambdaSingleBranchQuery', 'lambdaAllBranchesQuery', 'DfErrorException', 'refreshGz',
           'getAllGz']

# Cell
from .helper import DatabaseHelper
from .s3 import DatabaseS3
from .query import Querier
from .update import Updater
import pandas as pd
from datetime import datetime
from pynamodb.models import Model
from pynamodb.attributes import UnicodeAttribute, NumberAttribute, JSONAttribute, BooleanAttribute, BinaryAttribute, Attribute
from pynamodb.indexes import GlobalSecondaryIndex, AllProjection
from pynamodb.constants import BINARY
from awsSchema.apigateway import Response, Event
from botocore.config import Config
from s3bz.s3bz import S3
from linesdk.slack import SlackBot
from pprint import pprint
from nicHelper.wrappers import add_class_method, add_method
from nicHelper import pdUtils
from nicHelper.pdUtils import getDfHash
from nicHelper.dictUtil import hashDict
from nicHelper.timer import Timer
from beartype import beartype
from io import BytesIO
from typing import List
from s3bz.s3bz import ExtraArgs

import pickle, json, boto3, bz2, requests, validators, os, logging, sys, zlib

# Cell

DEFAULTCOLS = json.loads(os.environ.get('DEFAULTCOLS') or '[]')
try:
  SLACK = os.environ.get('SLACK')
  DATABASE_TABLE_NAME = os.environ['DATABASE_TABLE_NAME']
  PRICE_BUCKET_NAME = os.environ['PRICE_BUCKET_NAME']
  INPUT_BUCKET_NAME = os.environ['INPUT_BUCKET_NAME']
  REGION = os.environ['REGION']
  ACCESS_KEY_ID = None
  SECRET_ACCESS_KEY = None
except Exception as e:
  print(f'error, missing environment variables \n{e}')
  DATABASE_TABLE_NAME = None
  INVENTORY_BUCKET_NAME = None
  INPUT_BUCKET_NAME = None
  ACCESS_KEY_ID = None
  SECRET_ACCESS_KEY = None
  REGION = 'ap-southeast-1'
try:
  DAX_ENDPOINT = os.environ['DAX_ENDPOINT']
except:
  DAX_ENDPOINT = None
  print('dax endpoint missing')


# Cell
# dont forget to import dependent classes from the relevant notebooks
class PandasDataFrameAttribute(Attribute):
  attr_type = BINARY
  def filterData (self, df: pd.DataFrame)->pd.DataFrame:
    df = df.reindex(DEFAULTCOLS, axis=1)
    df['last_price'] = df['last_price'].fillna(df['price'])
    return df

  def serialize(self, value: pd.DataFrame)->bin:
    bio = BytesIO()
    floatVal:pd.DataFrame = value.astype(float)
    floatVal = self.filterData(floatVal)
    floatVal.reset_index(drop=True).to_feather(bio)
    data:bin = bio.getvalue()
    compressedData:bin = zlib.compress(data)
    return compressedData
  def deserialize(self, compressedData: bin)->pd.DataFrame:
    data = zlib.decompress(compressedData)
    bio = BytesIO(data)
    df: pd.DataFrame = pd.read_feather(bio)
    df = self.filterData(df)
    return df

class Database(Model):
  class Meta:
    table_name = DATABASE_TABLE_NAME
    region = REGION
    billing_mode='PAY_PER_REQUEST'
    dax_read_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None
    dax_write_endpoints = [DAX_ENDPOINT] if DAX_ENDPOINT else None

  brcode = UnicodeAttribute(hash_key=True, default = 0)
  lastUpdate = NumberAttribute(default=datetime.now().timestamp())
  data = PandasDataFrameAttribute()

  def __repr__(self):
    return self.data.head().to_string()
  def size(self):
    return f'data is size {sys.getsizeof(self.data)/1e3} kB'





# Cell
@add_class_method(Database)
def updateWithDfs(cls, df: pd.DataFrame, defaultDf= pd.DataFrame({i:[] for i in DEFAULTCOLS})):
  results = []
  ##### split df based on brcode
  fdf = {}
  brcodes = df['brcode'].unique()
  for brcode in brcodes:
    mask = df['brcode'] == brcode
    fdf[brcode] = df[mask]

  ##### loop through each df and update in the database
  with cls.batch_write() as batch:
    for brcode, data in fdf.items():
      db:cls = next(cls.query(str(brcode)),None) or cls(brcode=str(brcode), data=defaultDf)
      db.lastUpdate = datetime.now().timestamp()
#       print(db.data)
      df0 = db.data.set_index('cprcode')
      df1 = data.set_index('cprcode')
      try:
        df2 = df0.append(df1)
        df2['last_price'] = df2['last_price'].fillna(df2['price'])
        df2 = df2.astype(int)
        df2 = df2.reset_index().drop_duplicates(subset=['cprcode', 'brcode'], keep='last')
      except:
        print(f'error \n{df0.combine_first(df1)}')
        return False
      db.data = df2.reset_index(drop=True)
      print(db.size())
      result = batch.save(db)
  return True

# Cell
def lambdaPresignUpload(event, *args):
  key:str = Event.parseBody(event)['key']
  presigned:dict = S3.presignUpload(bucket=PRICE_BUCKET_NAME, key = key, expiry = 1000, conditions=[])
  return Response.returnSuccess(body = presigned)

# Cell
def lambdaIngestUpload(event, *args):
  body = Event.parseBody(event)
  key = body['key']
  dtype = body.get('dtype') or 'json'
  path = '/tmp/input'
  S3.loadFile(key = key, path = path, bucket = PRICE_BUCKET_NAME)
  if dtype == 'json':
    df = pd.read_json(path, dtype=str)
    print(f'reading jsoon {df.head()}')
  if dtype == 'feather':
    df = pd.read_feather(path)
    print(f'reading feather{df.head()}')
  result = Database.updateWithDfs(df=df)
  return Response.returnSuccess(df.head().to_dict())

# Cell
def lambdaSingleBranchQuery(event:dict, *args):
  def queryDatabase(brcode:str)->pd.DataFrame:
    df:pd.DataFrame = next(Database.query(brcode), Database(data=pd.DataFrame(dtype=int))).data
    return df

  def filterDb(cprcodes:List[int], df:pd.DataFrame):
    '''filter database based on the cprcodes'''
    filteredDf:pd.DataFrame = df[df['cprcode'].isin(cprcodes)]
    return filteredDf

  def saveDbToFile(df:pd.DataFrame, path:str, format_:str):
    if format_ == 'feather': df.to_feather(path=path)
    else: dbDict:dict = df.to_json(path,  orient='split')

  def saveFileToS3(key:str, path:str, bucket:str=PRICE_BUCKET_NAME):
    S3.saveFile(key=key,path=path,bucket=PRICE_BUCKET_NAME)

  ##### setup
  path:str = '/tmp/tempFile'
  body:dict = Event.parseBody(event)
  key:str = hashDict(body)
  brcode:str = str(body['brcode'])
  cprcodes:List[int] = [int(i) for i in body.get('cprcodes')]
  format_:str = body.get('format') or 'json'

  ###### main #####
  df = queryDatabase(brcode) # get data from db
  if cprcodes:
    df = filterDb(cprcodes, df) # filter the data
  saveDbToFile(df, path, format_) # save the df to file
  saveFileToS3(key, path) # move local file to s3
  url = S3.presign(key=key,path=path,bucket=PRICE_BUCKET_NAME) #presign the saved file

  return Response.returnSuccess({'url': url}) # retur the url

# Cell
from nicHelper.dictUtil import hash_dict
from cachetools.func import ttl_cache
from s3bz.s3bz import ExtraArgs

@hash_dict
@ttl_cache(ttl=100)
def lambdaAllBranchesQuery(event, *args):
  def getDb():
    result = Database.scan()
    dfs = [i.data for i in result]
    return dfs
  def mergeDf(dfs:List[pd.DataFrame])->pd.DataFrame:
    df:pd.DataFrame = pd.concat(dfs).astype(int).reset_index(drop=True)
    df = df.drop_duplicates(subset=['cprcode', 'brcode'],keep='last')
    return df
  def filterDf(df:pd.DataFrame, cprcodes:List[int]):
    fdf:pd.DataFrame = df[df['cprcode'].isin(cprcodes)]
    return fdf


  def dfToUrl(df:pd.DataFrame, path:str, key:str, format_:str, bucket:str):
    '''save df to s3 then create a presigned link'''
    def saveDbToFile(df:pd.DataFrame, path:str, format_:str):
      if format_ == 'feather':
        df.reset_index(drop=True).to_feather(path=path)
      else: dbDict:dict = df.to_json(path,  orient='split')

    def saveFileToS3(key:str, path:str, bucket:str=PRICE_BUCKET_NAME):
      S3.saveFile(key=key,path=path,bucket=PRICE_BUCKET_NAME)

    ### main ###
    saveDbToFile(df, path, format_)
    saveFileToS3(key, path, bucket=bucket)
    url = S3.presign(key=key,path=path,bucket=bucket)
    return url

  @beartype
  def dfToGzUrl(df:pd.DataFrame, path:str, key:str, bucket:str)->str:
    newKey:str = f'{key}.gz'
    df.to_json(path,  orient='split', compression ='gzip')
    S3.saveFile(key=newKey,path=path,bucket=bucket,
            ExtraArgs = {**ExtraArgs.gzip})
    return S3.presign(key=newKey, bucket=bucket)




  #### setup
  body:dict = Event.from_dict(event).getBody()
  if 'cprcodes' in body:
    cprcodes:List[int] = [int(i) for i in body.get('cprcodes')]
  else:
    cprcodes = []
  outputFormat:str = body.get('format') or 'json'
  key:str = hashDict(body)
  path = '/tmp/tmpFile'
  presignOnly:bool = body.get('presignOnly')
  bucket:str = PRICE_BUCKET_NAME




  #### main ######
  if presignOnly:
    newKey:str = f'{key}{".gz" if outputFormat == "gzip" else ""}'
    print(f'key is {newKey}')
    if S3.exist(newKey, bucket = bucket):
      url = S3.presign(key = newKey,bucket = bucket)
      return Response.returnSuccess(body = {'url':url})



  dfs:List[pd.DataFrame] = getDb()
  df:pd.DataFrame = mergeDf(dfs)
  if cprcodes: # filter df is cprcode exists
    df = filterDf(df, cprcodes)
  if outputFormat == 'gzip':
    url = dfToGzUrl(df=df, path=path, key=key, bucket=bucket) # push df to url
  else:
    url = dfToUrl(df=df, path=path, key=key, format_=outputFormat, bucket=bucket) # push df to url

  return Response.returnSuccess(body = {'url':url})


# Cell
class DfErrorException(Exception): pass

def refreshGz(*args):

  def saveToS3Workround(df:pd.DataFrame, bucket:str, brcode):
    df.to_json( f'/tmp/{brcode}.gz',orient = 'split', compression = 'gzip' )
    s3 = boto3.client('s3')
    s3.upload_file(f'/tmp/{brcode}.gz', bucket, f'{brcode}.gz',ExtraArgs = {**ExtraArgs.gzip})
  def testDfBeforeSaving(df):
    score = 0
    score += df.shape[0] > 4e4 # over 40k products
    score += sum(i in df.columns for i in ['cprcode', 'brcode', 'price', 'last_price'] )
    return score >= 5
  def getDb():
    result = Database.scan()
    dfs = [i.data for i in result]
    return dfs
  def mergeDf(dfs:List[pd.DataFrame])->pd.DataFrame:
    df:pd.DataFrame = pd.concat(dfs).astype(int).reset_index(drop=True)
    df = df.drop_duplicates(subset=['cprcode', 'brcode'],keep='last')
    return df
  def saveEachBranch(dfs:List[pd.DataFrame]):
    timer = Timer()
    for df in dfs:
      brcode:int = int(df.loc[0,'brcode'])
      if testDfBeforeSaving(df):
        saveToS3Workround(df, bucket = PRICE_BUCKET_NAME, brcode=brcode)
      else:
        print(f"error checking df {brcode}, {df.head()}")
#       S3.saveGz(bucket = PRICE_BUCKET_NAME, key = f'{brcode}.gz',
#                 item = df.to_dict(orient = 'split'))
    timer.print_reset('saving took')

  dfs: List[pd.DataFrame] = getDb()
  saveEachBranch(dfs)
  df: pd.DataFrame = mergeDf(dfs) # get all data df
  if testDfBeforeSaving(df):
    saveToS3Workround(df=df, bucket=PRICE_BUCKET_NAME, brcode='allData')
  else:
    raise DfErrorException(f'df didnt pass the test {df}')
#   S3.saveGz(bucket = PRICE_BUCKET_NAME, key = 'allData.gz', item = df.to_dict(orient = 'split'))
  return Response.returnSuccess()


# Cell
from nicHelper.dictUtil import hashDict
@hash_dict
@ttl_cache(ttl=100)
def getAllGz(event, *args):
  body:dict = Event.parseBody(event)
  brcode:str = str(body.get('brcode') or 'allData')
  url = S3.presign(key = f'{brcode}.gz',bucket=PRICE_BUCKET_NAME)
  return Response.returnSuccess(
    body = {'url':url},
    headers = {
      'Cache-Control': 'max-age=300',
      'Content-Type': 'application/json',
      'Access-Control-Allow-Headers': '*',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': '*',
    }
  )