{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "from datgan import stats_assessment\n",
    "from datgan import ml_assessment, transform_results\n",
    "\n",
    "# For the Python notebook\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the original and synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('./data/CMAP.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = [\"distance\", \"age\", \"departure_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected synthetic datasets generated from the CMAP datasets using different state-of-the-art models to be tested against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_files = ['data/TGAN.csv',\n",
    "               'data/CTGAN.csv',\n",
    "               'data/CTABGAN.csv',\n",
    "               'data/TVAE.csv',\n",
    "               'data/DATGAN.csv',\n",
    "               'data/DATGAN_ci.csv'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = './results/'\n",
    "\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_str = ['mae', 'rmse', 'r2', 'srmse', 'corr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical assessment\n",
    "\n",
    "The statistical assessment can be done on three aggregation levels. We encourage you to save results regularly since it can take quite some time to compute the statistical assessments, especially on the third level of aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous results found, starting fresh\n"
     ]
    }
   ],
   "source": [
    "pickle_name = 'stats_first_level.pickle'\n",
    "aggregation_level = 1\n",
    "\n",
    "first_lvl_stats = {}\n",
    "\n",
    "try:\n",
    "    first_lvl_stats = pickle.load(open(results_path + pickle_name, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for file \u001B[1mTGAN\u001B[0m (1/6)\n",
      "Preparing stats for file \u001B[1mCTGAN\u001B[0m (2/6)\n",
      "Preparing stats for file \u001B[1mCTABGAN\u001B[0m (3/6)\n",
      "Preparing stats for file \u001B[1mTVAE\u001B[0m (4/6)\n",
      "Preparing stats for file \u001B[1mDATGAN\u001B[0m (5/6)\n",
      "Preparing stats for file \u001B[1mDATGAN_ci\u001B[0m (6/6)\n",
      "\u001B[1mFINISHED!\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(synth_files):\n",
    "    \n",
    "    file_name = f.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    if file_name in first_lvl_stats:\n",
    "        print(\"Results for file \\033[1m{}\\033[0m ({}/{}) already exists!\".format(file_name, i+1, len(synth_files)))\n",
    "    else:\n",
    "        print(\"Preparing stats for file \\033[1m{}\\033[0m ({}/{})\".format(file_name, i+1, len(synth_files)))\n",
    "\n",
    "        first_lvl_stats[file_name] = {}\n",
    "        \n",
    "        df_synth = pd.read_csv(f, index_col=False)\n",
    "                \n",
    "        stats = stats_assessment(df_orig, df_synth, continuous_columns, aggregation_level)\n",
    "        \n",
    "        first_lvl_stats[file_name] = stats\n",
    "        \n",
    "    pickle.dump(first_lvl_stats, open(results_path + pickle_name, 'wb'))   \n",
    "    \n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-readable results\n",
    "\n",
    "This is an example how to obtain more human-readable results. With the first level of aggregation, we can check the difference between continuous and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    res[test] = {}\n",
    "    \n",
    "    if test == 'all':\n",
    "        cols = df_orig.columns\n",
    "    elif test == 'cont':\n",
    "        cols = continuous_columns\n",
    "    elif test == 'cat':\n",
    "        cols = set(df_orig.columns) - set(continuous_columns)\n",
    "        \n",
    "    for s in stats_str:\n",
    "        res[test][s] = {}\n",
    "        \n",
    "    for m in first_lvl_stats.keys():\n",
    "\n",
    "        for s in stats_str:\n",
    "            res[test][s][m] = []\n",
    "            \n",
    "            for c in cols:\n",
    "                res[test][s][m].append(first_lvl_stats[m][c][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    avg[test] = {}\n",
    "\n",
    "    for s in stats_str:\n",
    "        avg[test][s] = {}\n",
    "\n",
    "        for m in first_lvl_stats.keys():\n",
    "            avg[test][s][m] = {\n",
    "                'mean': np.mean(res[test][s][m]),\n",
    "                'std': np.std(res[test][s][m])\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking on all columns based on SRMSE:\n",
      "   1. DATGAN_ci       - 5.42e-02 ± 5.56e-02\n",
      "   2. DATGAN          - 6.02e-02 ± 5.58e-02\n",
      "   3. TGAN            - 1.34e-01 ± 1.40e-01\n",
      "   4. CTABGAN         - 2.13e-01 ± 1.38e-01\n",
      "   5. TVAE            - 2.32e-01 ± 1.64e-01\n",
      "   6. CTGAN           - 2.34e-01 ± 1.08e-01\n",
      "\n",
      "Ranking on continuous columns based on SRMSE:\n",
      "   1. CTABGAN         - 1.20e-01 ± 6.96e-02\n",
      "   2. DATGAN_ci       - 1.25e-01 ± 7.63e-02\n",
      "   3. DATGAN          - 1.35e-01 ± 8.49e-02\n",
      "   4. TVAE            - 1.61e-01 ± 3.52e-02\n",
      "   5. CTGAN           - 1.87e-01 ± 1.01e-01\n",
      "   6. TGAN            - 4.04e-01 ± 2.12e-02\n",
      "\n",
      "Ranking on categorical columns based on SRMSE:\n",
      "   1. DATGAN_ci       - 3.64e-02 ± 2.87e-02\n",
      "   2. DATGAN          - 4.13e-02 ± 1.77e-02\n",
      "   3. TGAN            - 6.61e-02 ± 3.94e-02\n",
      "   4. CTABGAN         - 2.36e-01 ± 1.41e-01\n",
      "   5. CTGAN           - 2.45e-01 ± 1.07e-01\n",
      "   6. TVAE            - 2.49e-01 ± 1.78e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in ['all', 'cont', 'cat']:\n",
    "    \n",
    "    if test == 'all':\n",
    "        str_ = 'on all columns'\n",
    "    elif test == 'cont':\n",
    "        str_ = 'on continuous columns'\n",
    "    elif test == 'cat':\n",
    "        str_ = 'on categorical columns'\n",
    "        \n",
    "    for s in ['srmse']:#stats:\n",
    "        print('Ranking {} based on {}:'.format(str_, s.upper()))\n",
    "\n",
    "        if s in ['r2', 'corr']:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "        else:\n",
    "            sorted_dct = {k: v for k, v in sorted(avg[test][s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "        for i, item in enumerate(sorted_dct):\n",
    "            print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second and third level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous results found, starting fresh\n"
     ]
    }
   ],
   "source": [
    "pickle_name = 'stats_second_level.pickle'\n",
    "aggregation_level = 2\n",
    "\n",
    "second_lvl_stats = {}\n",
    "\n",
    "try:\n",
    "    second_lvl_stats = pickle.load(open(results_path + pickle_name, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for file \u001B[1mTGAN\u001B[0m (1/6)\n",
      "Preparing stats for file \u001B[1mCTGAN\u001B[0m (2/6)\n",
      "Preparing stats for file \u001B[1mCTABGAN\u001B[0m (3/6)\n",
      "Preparing stats for file \u001B[1mTVAE\u001B[0m (4/6)\n",
      "Preparing stats for file \u001B[1mDATGAN\u001B[0m (5/6)\n",
      "Preparing stats for file \u001B[1mDATGAN_ci\u001B[0m (6/6)\n",
      "\u001B[1mFINISHED!\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(synth_files):\n",
    "    \n",
    "    file_name = f.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    if file_name in second_lvl_stats:\n",
    "        print(\"Results for file \\033[1m{}\\033[0m ({}/{}) already exists!\".format(file_name, i+1, len(synth_files)))\n",
    "    else:\n",
    "        print(\"Preparing stats for file \\033[1m{}\\033[0m ({}/{})\".format(file_name, i+1, len(synth_files)))\n",
    "\n",
    "        second_lvl_stats[file_name] = {}\n",
    "        \n",
    "        df_synth = pd.read_csv(f, index_col=False)\n",
    "                \n",
    "        stats = stats_assessment(df_orig, df_synth, continuous_columns, aggregation_level)\n",
    "        \n",
    "        second_lvl_stats[file_name] = stats\n",
    "        \n",
    "    pickle.dump(second_lvl_stats, open(results_path + pickle_name, 'wb'))  \n",
    "    \n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-readable results\n",
    "\n",
    "As for the first aggregation level, we can make the results more human-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "        \n",
    "for s in stats_str:\n",
    "    res[s] = {}\n",
    "\n",
    "for m in second_lvl_stats.keys():\n",
    "\n",
    "    for s in stats_str:\n",
    "        res[s][m] = []\n",
    "\n",
    "        for c in second_lvl_stats[m].keys():\n",
    "            res[s][m].append(second_lvl_stats[m][c][s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {}\n",
    "\n",
    "for s in stats_str:\n",
    "    avg[s] = {}\n",
    "\n",
    "    for m in first_lvl_stats.keys():\n",
    "        avg[s][m] = {\n",
    "            'mean': np.mean(res[s][m]),\n",
    "            'std': np.std(res[s][m])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking based on SRMSE for aggregation level 2:\n",
      "   1. DATGAN_ci       - 1.63e-01 ± 1.05e-01\n",
      "   2. DATGAN          - 1.66e-01 ± 1.06e-01\n",
      "   3. TGAN            - 3.40e-01 ± 2.45e-01\n",
      "   4. CTABGAN         - 4.88e-01 ± 2.14e-01\n",
      "   5. CTGAN           - 5.09e-01 ± 1.69e-01\n",
      "   6. TVAE            - 5.62e-01 ± 2.88e-01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in ['srmse']:#stats:\n",
    "    print('Ranking based on {} for aggregation level {}:'.format(s.upper(), aggregation_level))\n",
    "\n",
    "    if s in ['r2', 'corr']:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])[::-1]}\n",
    "    else:\n",
    "        sorted_dct = {k: v for k, v in sorted(avg[s].items(), key=lambda item: item[1]['mean'])}\n",
    "\n",
    "    for i, item in enumerate(sorted_dct):\n",
    "        print('  {:>2}. {:<15} - {:.2e} ± {:.2e}'.format(i+1, item, sorted_dct[item]['mean'], sorted_dct[item]['std']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing for the third level of aggregation. We just don't show it here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning efficacy\n",
    "\n",
    "For this test, you first need to check if there are some values that appear with low probability. If it's the case, we recommend to replace these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_low_appearing_vars(df):\n",
    "    \n",
    "    for c in df.columns:\n",
    "        val = df[c].value_counts()\n",
    "        if len(val) < 20:\n",
    "            val = val/len(df)\n",
    "            if any(val < 0.01) and c != 'choice':\n",
    "                print('Variable {}: '.format(c))\n",
    "                for idx, v in zip(val.index, val):\n",
    "                    if v < 0.01:\n",
    "                        print('  {} - {:.2f}% ({:d})'.format(idx, 100*v, int(v*len(df))))\n",
    "                print()\n",
    "                \n",
    "def replace_low_appearing_values(df):\n",
    "    \n",
    "    dct_ = {}\n",
    "    for i in df['hh_vehicles'].unique():\n",
    "        if i >= 5:\n",
    "            dct_[i] = '5+'\n",
    "        else:\n",
    "            dct_[i] = str(i)        \n",
    "    df['hh_vehicles'].replace(dct_, inplace=True)\n",
    "\n",
    "    dct_ = {}\n",
    "    for i in df['hh_size'].unique():\n",
    "        if i >= 6:\n",
    "            dct_[i] = '6+'\n",
    "        else:\n",
    "            dct_[i] = str(i)        \n",
    "    df['hh_size'].replace(dct_, inplace=True)\n",
    "\n",
    "    dct_ = {}\n",
    "    for i in df['hh_bikes'].unique():\n",
    "        if i >= 6:\n",
    "            dct_[i] = '6+'\n",
    "        else:\n",
    "            dct_[i] = str(i)        \n",
    "    df['hh_bikes'].replace(dct_, inplace=True)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable hh_vehicles: \n",
      "  5 - 0.67% (60)\n",
      "  6 - 0.26% (23)\n",
      "  7 - 0.08% (7)\n",
      "  8 - 0.06% (5)\n",
      "\n",
      "Variable hh_size: \n",
      "  7 - 0.36% (32)\n",
      "  8 - 0.13% (12)\n",
      "\n",
      "Variable hh_bikes: \n",
      "  6 - 0.77% (69)\n",
      "  7 - 0.27% (24)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_low_appearing_vars(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_low_appearing_values(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_low_appearing_vars(df_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed the low appearing values => we can continue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns\n",
    "categorical_columns = list(set(df_orig.columns) - set(continuous_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()\n",
    "df_orig[categorical_columns] = enc.fit_transform(df_orig[categorical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the statistical tests, we recommend to save the files between each synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous results found, starting fresh\n"
     ]
    }
   ],
   "source": [
    "pickle_name = 'ml_efficacy.pickle'\n",
    "\n",
    "cv_modelscores = {}\n",
    "\n",
    "try:\n",
    "    cv_modelscores = pickle.load(open(results_path + pickle_name, 'rb'))\n",
    "    print('Found previous pickel file, using that')\n",
    "except:\n",
    "    print('No previous results found, starting fresh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for file \u001B[1mTGAN\u001B[0m (1/6)\n",
      "Preparing stats for file \u001B[1mCTGAN\u001B[0m (2/6)\n",
      "Preparing stats for file \u001B[1mCTABGAN\u001B[0m (3/6)\n",
      "Preparing stats for file \u001B[1mTVAE\u001B[0m (4/6)\n",
      "Preparing stats for file \u001B[1mDATGAN\u001B[0m (5/6)\n",
      "Preparing stats for file \u001B[1mDATGAN_ci\u001B[0m (6/6)\n",
      "\u001B[1mFINISHED!\u001B[0m                  \n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(synth_files):\n",
    "    \n",
    "    file_name = f.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    if file_name in cv_modelscores:\n",
    "        print(\"Results for file \\033[1m{}\\033[0m ({}/{}) already exists!\".format(file_name, i+1, len(synth_files)))\n",
    "    else:\n",
    "        print(\"Preparing stats for file \\033[1m{}\\033[0m ({}/{})\".format(file_name, i+1, len(synth_files)))\n",
    "\n",
    "        cv_modelscores[file_name] = {}\n",
    "        \n",
    "        # Load the synthetic dataset\n",
    "        df_synth = pd.read_csv(f, index_col=False)\n",
    "        \n",
    "        # Replace the values rarely appearing\n",
    "        replace_low_appearing_values(df_synth)\n",
    "        \n",
    "        # Encode the synthetic dataset\n",
    "        df_synth[categorical_columns] = enc.transform(df_synth[categorical_columns])\n",
    "                \n",
    "        res = ml_assessment(df_orig, df_synth, continuous_columns, categorical_columns)\n",
    "        \n",
    "        cv_modelscores[file_name] = res\n",
    "        \n",
    "    pickle.dump(cv_modelscores, open(results_path + pickle_name, 'wb'))   \n",
    "    \n",
    "print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw results are a bit difficult to assess. Therefore, we provide a way to get an ordered list of the synthetic datasets tested. However, for this, you need to run the ML assessment on the original dataset with a specific key!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing stats for file \u001B[1moriginal\u001B[0m\n",
      "\u001B[1mFINISHED!\u001B[0m                  \n"
     ]
    }
   ],
   "source": [
    "if 'original' in cv_modelscores:\n",
    "    print(\"Results for file \\033[1m{}\\033[0m already exists!\".format('original'))\n",
    "else:\n",
    "    print(\"Preparing stats for file \\033[1m{}\\033[0m\".format('original'))\n",
    "\n",
    "    res = ml_assessment(df_orig, df_orig, continuous_columns, categorical_columns)\n",
    "    cv_modelscores['original'] = res\n",
    "    pickle.dump(cv_modelscores, open(results_path + pickle_name, 'wb'))   \n",
    "    print(\"\\033[1mFINISHED!\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_sorted, cat_sorted = transform_results(cv_modelscores, continuous_columns, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | categorical                    | continuous                    \n",
      "-----------------------------------------------------------\n",
      " 1 | original    : -2.213           | original    : 2.573           \n",
      " 2 | DATGAN_ci   : 0.782            | DATGAN_ci   : 3.093           \n",
      " 3 | DATGAN      : 0.790            | DATGAN      : 3.108           \n",
      " 4 | TGAN        : 1.022            | CTABGAN     : 3.242           \n",
      " 5 | CTGAN       : 1.486            | TGAN        : 3.264           \n",
      " 6 | CTABGAN     : 1.539            | TVAE        : 3.286           \n",
      " 7 | TVAE        : 4.427            | CTGAN       : 3.332           \n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "print('   | {:<30} | {:<30}'.format('categorical', 'continuous'))\n",
    "print('-----------------------------------------------------------')\n",
    "for a, b in zip(cat_sorted, cont_sorted):\n",
    "    print('{:>2} | {:<30} | {:<30}'.format(i, '{:<12}: {:.3f}'.format(a[0], a[1]), '{:<12}: {:.3f}'.format(b[0], b[1])))\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}