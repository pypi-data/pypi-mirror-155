{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b097506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bw2data as bd\n",
    "import bw2calc as bc\n",
    "import bw_processing as bwp\n",
    "from pypardiso import spsolve\n",
    "from pathlib import Path\n",
    "from gsa_framework.utils import read_pickle, write_pickle\n",
    "from gsa_framework.visualization.plotting import plot_histogram_Y1_Y2\n",
    "from fs.zipfs import ZipFS\n",
    "from copy import deepcopy\n",
    "from consumption_model_ch.utils import get_habe_filepath\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('/Users/akim/PycharmProjects/akula')\n",
    "from collections import Counter\n",
    "\n",
    "from akula.sensitivity_analysis import get_mask\n",
    "\n",
    "project = 'GSA for archetypes'\n",
    "bd.projects.set_current(project)\n",
    "iterations = 2000\n",
    "seed = 11111000 \n",
    "\n",
    "fp_paper_2 = Path(\"write_files\") / project.lower().replace(\" \", \"_\") / \"paper_2\"\n",
    "fp_paper_2.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = bd.Database('swiss consumption 1.0')\n",
    "fu = [act for act in co if \"Food\" in act['name']][0]\n",
    "demand = {fu: 1}\n",
    "method = (\"IPCC 2013\", \"climate change\", \"GWP 100a\", \"uncertain\")\n",
    "fu_mapped, packages, _ = bd.prepare_lca_inputs(demand=demand, method=method, remapping=False)\n",
    "lca = bc.LCA(demand=fu_mapped, data_objs=packages, use_distributions=False)\n",
    "lca.lci()\n",
    "lca.lcia()\n",
    "lca.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = bd.Database('biosphere3').datapackage()\n",
    "ei = bd.Database('ecoinvent 3.8 cutoff').datapackage()\n",
    "co = bd.Database('swiss consumption 1.0').datapackage()\n",
    "me = bd.Method(method).datapackage()\n",
    "\n",
    "co_indices = co.get_resource(\"swiss_consumption_1.0_technosphere_matrix.indices\")[0]\n",
    "co_flip = co.get_resource(\"swiss_consumption_1.0_technosphere_matrix.flip\")[0]\n",
    "# co_vector = co.get_resource(\"swiss_consumption_1.0_technosphere_matrix.data\")[0]\n",
    "# co_data = np.random.rand(len(co_vector), iterations)\n",
    "# get_co_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_household_data(indices, co_name=\"swiss consumption 1.0\"):\n",
    "    # 1. Get some metadata from the consumption database\n",
    "    co = bd.Database(co_name)\n",
    "    year_habe = co.metadata['year_habe']\n",
    "    dir_habe = co.metadata['dir_habe']\n",
    "\n",
    "    # 2. Extract total demand from HABE\n",
    "    path_ausgaben = get_habe_filepath(dir_habe, year_habe, 'Ausgaben')\n",
    "    path_mengen = get_habe_filepath(dir_habe, year_habe, 'Mengen')\n",
    "    path_konsumgueter = get_habe_filepath(dir_habe, year_habe, 'Konsumgueter')\n",
    "\n",
    "    # change codes to be consistent with consumption database and Andi's codes\n",
    "    ausgaben = pd.read_csv(path_ausgaben, sep='\\t')\n",
    "    mengen = pd.read_csv(path_mengen, sep='\\t')\n",
    "    konsumgueter = pd.read_csv(path_konsumgueter, sep='\\t')\n",
    "    ausgaben.columns = [col.lower() for col in ausgaben.columns]\n",
    "    mengen.columns = [col.lower() for col in mengen.columns]\n",
    "    konsumgueter.columns = [col.lower() for col in konsumgueter.columns]\n",
    "    codes_co_db = sorted([act['code'] for act in co])\n",
    "    columns_a = ausgaben.columns.values\n",
    "    columns_m = [columns_a[0]]\n",
    "    columns_k = [columns_a[0]]\n",
    "    codes_m = []\n",
    "    for code_a in columns_a[1:]:\n",
    "        code_m = code_a.replace('a', 'm')\n",
    "        if code_m in codes_co_db:\n",
    "            columns_m.append(code_m)\n",
    "            codes_m.append(code_m)\n",
    "        else:\n",
    "            columns_m.append(code_a)\n",
    "    ausgaben.columns = columns_m\n",
    "    # Replace ausgaben data with mengen data\n",
    "    for code_m in codes_m:\n",
    "        ausgaben[code_m] = mengen[code_m].values\n",
    "\n",
    "    data = np.zeros((0,len(ausgaben)))\n",
    "    for inds in indices:\n",
    "        input_code = bd.get_activity(inds[0])['code']\n",
    "        try:\n",
    "            data_current = ausgaben[input_code].values\n",
    "            data = np.vstack([data, data_current])\n",
    "        except:\n",
    "            print(input_code)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foreground uncertainty weighted by number of people per household\n",
    "def get_household_data_weighted(indices, co_name=\"swiss consumption 1.0\"):\n",
    "    # 1. Get some metadata from the consumption database\n",
    "    co = bd.Database(co_name)\n",
    "    year_habe = co.metadata['year_habe']\n",
    "    dir_habe = co.metadata['dir_habe']\n",
    "\n",
    "    # 2. Extract total demand from HABE\n",
    "    path_ausgaben = get_habe_filepath(dir_habe, year_habe, 'Ausgaben')\n",
    "    path_mengen = get_habe_filepath(dir_habe, year_habe, 'Mengen')\n",
    "    path_konsumgueter = get_habe_filepath(dir_habe, year_habe, 'Konsumgueter')\n",
    "    path_personen = get_habe_filepath(dir_habe, year_habe, 'Personen')\n",
    "\n",
    "    # change codes to be consistent with consumption database and Andi's codes\n",
    "    ausgaben = pd.read_csv(path_ausgaben, sep='\\t')\n",
    "    mengen = pd.read_csv(path_mengen, sep='\\t')\n",
    "    konsumgueter = pd.read_csv(path_konsumgueter, sep='\\t')\n",
    "    personen_raw = pd.read_csv(path_personen, sep='\\t')\n",
    "    ausgaben.columns = [col.lower() for col in ausgaben.columns]\n",
    "    mengen.columns = [col.lower() for col in mengen.columns]\n",
    "    konsumgueter.columns = [col.lower() for col in konsumgueter.columns]\n",
    "    personen_raw.columns = [col.lower() for col in personen_raw.columns]\n",
    "    num_personen = dict(Counter(personen_raw[\"haushaltid\"]))\n",
    "    num_personen = [{'haushaltid': k, \"n_personen\": v} for k,v in num_personen.items()]\n",
    "    personen = pd.DataFrame.from_dict(num_personen)\n",
    "\n",
    "    codes_co_db = sorted([act['code'] for act in co])\n",
    "    columns_a = ausgaben.columns.values\n",
    "    columns_m = [columns_a[0]]\n",
    "    columns_k = [columns_a[0]]\n",
    "    codes_m = []\n",
    "    for code_a in columns_a[1:]:\n",
    "        code_m = code_a.replace('a', 'm')\n",
    "        if code_m in codes_co_db:\n",
    "            columns_m.append(code_m)\n",
    "            codes_m.append(code_m)\n",
    "        else:\n",
    "            columns_m.append(code_a)\n",
    "    ausgaben.columns = columns_m\n",
    "    # Replace ausgaben data with mengen data\n",
    "    for code_m in codes_m:\n",
    "        ausgaben[code_m] = mengen[code_m].values\n",
    "    weighted_ausgaben = pd.concat(\n",
    "        [\n",
    "            personen.set_index('haushaltid'),\n",
    "            ausgaben.set_index('haushaltid'), \n",
    "        ],\n",
    "        join='inner',\n",
    "        axis=1\n",
    "    )\n",
    "    weighted_ausgaben = weighted_ausgaben.iloc[:,1:].div(weighted_ausgaben['n_personen'], axis=0)\n",
    "    weighted_ausgaben = weighted_ausgaben.reset_index()\n",
    "\n",
    "    data = np.zeros((0,len(weighted_ausgaben)))\n",
    "    for inds in indices:\n",
    "        input_code = bd.get_activity(inds[0])['code']\n",
    "        try:\n",
    "            data_current = weighted_ausgaben[input_code].values\n",
    "            data = np.vstack([data, data_current])\n",
    "        except:\n",
    "            print(input_code)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_indices = [(exc.input.id, fu.id) for exc in fu.exchanges() if exc['type']!='production']\n",
    "use_mask = get_mask(co_indices, use_indices)\n",
    "use_flip = co_flip[use_mask]\n",
    "\n",
    "household_data = get_household_data(use_indices)\n",
    "choice = np.sort(np.random.choice(household_data.shape[1], iterations, replace=True))\n",
    "use_data = household_data[:, choice]\n",
    "\n",
    "household_data_weighted = get_household_data_weighted(use_indices)\n",
    "choice_weighted = np.sort(np.random.choice(household_data_weighted.shape[1], iterations, replace=True))\n",
    "use_data_weighted = household_data_weighted[:, choice]\n",
    "\n",
    "use_indices = np.array(use_indices, dtype=[('row', '<i4'), ('col', '<i4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_per_hh = 2.2415871421396285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_static = co.exclude({\"matrix\": 'technosphere_matrix'})\n",
    "co_static = co\n",
    "co_uncertain = bwp.create_datapackage(sequential=True)\n",
    "co_uncertain.add_persistent_array(\n",
    "    matrix=\"technosphere_matrix\",\n",
    "    indices_array=use_indices,\n",
    "    name=\"swiss_consumption_1.0_technosphere_matrix\",\n",
    "    data_array=use_data,\n",
    "    flip_array=use_flip,\n",
    ")\n",
    "co_uncertain_weighted = bwp.create_datapackage(sequential=True)\n",
    "co_uncertain_weighted.add_persistent_array(\n",
    "    matrix=\"technosphere_matrix\",\n",
    "    indices_array=use_indices,\n",
    "    name=\"swiss_consumption_1.0_weighted_technosphere_matrix\",\n",
    "    data_array=use_data_weighted,\n",
    "    flip_array=use_flip,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06750cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_static = me.exclude({\"kind\": \"distributions\"})\n",
    "ei_static = ei.exclude({\"kind\": \"distributions\"})\n",
    "bs_static = bs.exclude({\"kind\": \"distributions\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc04f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dps_bg = [me, bs, ei, co_static]\n",
    "dps_bg_fg = [me, bs, ei, co_static, co_uncertain]\n",
    "dps_fg = [me, bs, ei_static, co_static, co_uncertain]\n",
    "dps_bg_fg_weighted = [me, bs, ei, co_static, co_uncertain_weighted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_monte_carlo_bg_fg = fp_paper_2 / \"{}.{}.{}.pickle\".format(\n",
    "    \"bg+fg\", iterations, seed\n",
    ")\n",
    "fp_monte_carlo_bg_fg_weighted = fp_paper_2 / \"{}.{}.{}.pickle\".format(\n",
    "    \"bg+fg+weighted\", iterations, seed\n",
    ")\n",
    "fp_monte_carlo_bg = fp_paper_2 / \"{}.{}.{}.pickle\".format(\n",
    "    \"bg\", iterations, seed\n",
    ")\n",
    "fp_monte_carlo_fg = fp_paper_2 / \"{}.{}.{}.pickle\".format(\n",
    "    \"fg\", iterations, seed\n",
    ")\n",
    "\n",
    "options = {\n",
    "    \"bg+fg\": {\n",
    "        \"fp\": fp_monte_carlo_bg_fg,\n",
    "        \"dps\": dps_bg_fg,\n",
    "    },\n",
    "    \"bg\": {\n",
    "        \"fp\": fp_monte_carlo_bg,\n",
    "        \"dps\": dps_bg,\n",
    "    },\n",
    "    \"bg+fg+weighted\": {\n",
    "        \"fp\": fp_monte_carlo_bg_fg_weighted,\n",
    "        \"dps\": dps_bg_fg_weighted,\n",
    "    },\n",
    "#     \"fg\": {\n",
    "#         \"fp\": fp_monte_carlo_fg,\n",
    "#         \"dps\": dps_fg,\n",
    "#     }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85cecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = {}\n",
    "for option, data in options.items():\n",
    "    print(option)\n",
    "    fp = data['fp']\n",
    "    dps = data['dps']\n",
    "    if fp.exists():\n",
    "        scores[option] = read_pickle(fp)\n",
    "    else:\n",
    "        if option=='fg':\n",
    "            use_distributions = False\n",
    "        else:\n",
    "            use_distributions = True\n",
    "        dict_for_lca = dict(\n",
    "            use_distributions=use_distributions,\n",
    "            use_arrays=True,\n",
    "            seed_override=seed,\n",
    "        )\n",
    "        lca_new = bc.LCA(\n",
    "            fu_mapped,\n",
    "            data_objs=dps,\n",
    "            **dict_for_lca,\n",
    "        )\n",
    "        lca_new.lci()\n",
    "        lca_new.lcia()\n",
    "        scores_current = []\n",
    "        for i in range(iterations):\n",
    "            next(lca_new)\n",
    "            scores_current.append(lca_new.score)\n",
    "        scores[option] = scores_current\n",
    "        write_pickle(scores[option], fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38559cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores['bg+fg+weighted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da844dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "start = 0\n",
    "end = 200\n",
    "fig = go.Figure()\n",
    "for option, data in scores.items():\n",
    "    data = np.array(data)\n",
    "    if option == 'bg':\n",
    "        data /= ppl_per_hh\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.arange(iterations),\n",
    "            y=data[start:end],\n",
    "            name=option,\n",
    "            mode=\"lines+markers\",\n",
    "            showlegend=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(scores['bg+fg']), max(scores['bg+fg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_min = 0\n",
    "bin_max = 1000#max(scores['bg+fg'])\n",
    "num_bins = 150\n",
    "opacity = 0.65\n",
    "\n",
    "bins_ = np.linspace(bin_min, bin_max, num_bins, endpoint=True)\n",
    "color_bg = \"rgb(29,105,150)\"\n",
    "color_bg_fg = \"rgb(148, 52, 110)\" #\"rgb(36, 121, 108)\"\n",
    "color_gray_hex = \"#b2bcc0\"\n",
    "color_darkgray_hex = \"#485063\"\n",
    "color_black_hex = \"#212931\"\n",
    "lca_scores_axis_title = r\"$\\text{LCIA scores, [kg CO}_2\\text{-eq}]$\"\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Background + foreground\n",
    "freq1, bins1 = np.histogram(scores['bg+fg+weighted'], bins=bins_)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=bins1,\n",
    "        y=freq1,\n",
    "        name=r\"$\\text{Background and foreground vary}$\",\n",
    "        opacity=opacity,\n",
    "        line=dict(color=color_bg_fg, width=1, shape=\"hvh\"),\n",
    "        showlegend=True,\n",
    "        fill=\"tozeroy\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Background\n",
    "freq2, bins2 = np.histogram(np.array(scores['bg'])/ppl_per_hh, bins=bins_)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=bins2,\n",
    "        y=freq2,\n",
    "        name=r\"$\\text{Only background varies}$\",\n",
    "        opacity=opacity,\n",
    "        line=dict(color=color_bg, width=1, shape=\"hvh\"),\n",
    "        showlegend=True,\n",
    "        fill=\"tozeroy\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=lca_scores_axis_title,\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=color_gray_hex,\n",
    "    zeroline=True,\n",
    "    zerolinewidth=1,\n",
    "    zerolinecolor=color_gray_hex,\n",
    "    showline=True,\n",
    "    linewidth=1,\n",
    "    linecolor=color_gray_hex,\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    title_text=r\"$\\text{Frequency}$\", \n",
    "    range=[-10,550],\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=color_gray_hex,\n",
    "    zeroline=True,\n",
    "    zerolinewidth=1,\n",
    "    zerolinecolor=color_black_hex,\n",
    "    showline=True,\n",
    "    linewidth=1,\n",
    "    linecolor=color_gray_hex,\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=600,\n",
    "    height=250,\n",
    "    paper_bgcolor=\"rgba(255,255,255,1)\",\n",
    "    plot_bgcolor=\"rgba(255,255,255,1)\",\n",
    "    legend=dict(\n",
    "        x=0.7,\n",
    "        y=0.90,\n",
    "        orientation=\"v\",\n",
    "        xanchor=\"center\",\n",
    "        font=dict(size=12),\n",
    "        # bgcolor=color_lightgray_hex,\n",
    "        bordercolor=color_darkgray_hex,\n",
    "        borderwidth=1,\n",
    "    ),\n",
    "    margin=dict(l=65, r=0, t=0, b=0),\n",
    ")\n",
    "\n",
    "# filepath_fig = fp_paper_2 / \"lca_scores_uncertainty_bg_fg.pdf\"\n",
    "# fig.write_image(filepath_fig.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath_fig = Path(\"lca_scores_uncertainty_bg_fg_per_capita_impacts.pdf\")\n",
    "# fig.write_image(filepath_fig.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba627d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9682a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299f2175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af15a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584f591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5388a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5340419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a24ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
